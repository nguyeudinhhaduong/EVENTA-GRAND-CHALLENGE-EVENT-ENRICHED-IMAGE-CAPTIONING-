{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9faOe-6U1XzD"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93-wm0bz1XzF"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo2qD5_O1XzF"
      },
      "source": [
        "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
        "\n",
        "Read our **[Qwen3 Guide](https://docs.unsloth.ai/basics/qwen3-how-to-run-and-fine-tune)** and check out our new **[Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs)** quants which outperforms other quantization methods!\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0QQoNst1XzG"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpXTiV9t1XzG"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "    !pip install transformers==4.51.3\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xLDGk41C7IF"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmUBVEnvCDJv"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastVisionModel # FastLanguageModel for LLMs\n",
        "import torch\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit\", # Llama 3.2 vision support\n",
        "    \"unsloth/Llama-3.2-11B-Vision-bnb-4bit\",\n",
        "    \"unsloth/Llama-3.2-90B-Vision-Instruct-bnb-4bit\", # Can fit in a 80GB card!\n",
        "    \"unsloth/Llama-3.2-90B-Vision-bnb-4bit\",\n",
        "\n",
        "    \"unsloth/Pixtral-12B-2409-bnb-4bit\",              # Pixtral fits in 16GB!\n",
        "    \"unsloth/Pixtral-12B-Base-2409-bnb-4bit\",         # Pixtral base model\n",
        "\n",
        "    \"unsloth/Qwen2-VL-2B-Instruct-bnb-4bit\",          # Qwen2 VL support\n",
        "    \"unsloth/Qwen2-VL-7B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Qwen2-VL-72B-Instruct-bnb-4bit\",\n",
        "\n",
        "    \"unsloth/llava-v1.6-mistral-7b-hf-bnb-4bit\",      # Any Llava variant works!\n",
        "    \"unsloth/llava-1.5-7b-hf-bnb-4bit\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastVisionModel.from_pretrained(\n",
        "    \"unsloth/Qwen2.5-VL-7B-Instruct-bnb-4bit\",\n",
        "    load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters for parameter efficient finetuning - this allows us to only efficiently train 1% of all parameters.\n",
        "\n",
        "**[NEW]** We also support finetuning ONLY the vision part of the model, or ONLY the language part. Or you can select both! You can also select to finetune the attention or the MLP layers!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = FastVisionModel.get_peft_model(\n",
        "    model,\n",
        "    finetune_vision_layers     = True, # False if not finetuning vision layers\n",
        "    finetune_language_layers   = True, # False if not finetuning language layers\n",
        "    finetune_attention_modules = True, # False if not finetuning attention layers\n",
        "    finetune_mlp_modules       = True, # False if not finetuning MLP layers\n",
        "\n",
        "    r = 128,           # The larger, the higher the accuracy, but might overfit\n",
        "    lora_alpha = 16,  # Recommended alpha == r at least\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        "    # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep\n",
        "We'll be using a sampled dataset of handwritten maths formulas. The goal is to convert these images into a computer readable form - ie in LaTeX form, so we can render it. This can be very useful for complex formulas.\n",
        "\n",
        "You can access the dataset [here](https://huggingface.co/datasets/unsloth/LaTeX_OCR). The full dataset is [here](https://huggingface.co/datasets/linxy/LaTeX_OCR)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1W2Qhsz6rUT"
      },
      "source": [
        "Let's take an overview look at the dataset. We shall see what the 3rd image is, and what caption it had."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUmk0GcXDQBN"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wr0mqh6VuZKg"
      },
      "outputs": [],
      "source": [
        "/content/drive/MyDrive/eventa/query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUKL3b6Dt-XL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btUi40Bk6dzg"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Features, Image as HFImage, Value\n",
        "import json\n",
        "import os\n",
        "json_file_path = \"/content/drive/MyDrive/eventa/dataforQwen - Copy.json\"\n",
        "custom_features = Features({\n",
        "    'image': Value('string'),      # This will load the image from the path string\n",
        "    'instruction': Value('string'),\n",
        "    'output': Value('string'),\n",
        "})\n",
        "try:\n",
        "    dataset = load_dataset(\"json\", data_files=json_file_path, features=custom_features, split=\"train\")\n",
        "except Exception as e:\n",
        "    print(f\"Lỗi khi tải dataset: {e}\")\n",
        "    print(\"Vui lòng kiểm tra định dạng file JSON và đường dẫn hình ảnh.\")\n",
        "    exit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfcSGwIb6p_R"
      },
      "outputs": [],
      "source": [
        "subset = dataset.select(range(5000))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68fyvnth6dzg"
      },
      "outputs": [],
      "source": [
        "subset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hub1IesdCgY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision.transforms import ToTensor\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import gc\n",
        "import time\n",
        "\n",
        "# === Convert GPU Tensor -> CPU PIL Image ===\n",
        "def tensor_to_pil(tensor_img):\n",
        "    if tensor_img.is_cuda:\n",
        "        tensor_img = tensor_img.cpu()\n",
        "    tensor_img = tensor_img.mul(255).byte().clamp(0, 255)\n",
        "    return transforms.ToPILImage()(tensor_img)\n",
        "\n",
        "# === Load + resize ảnh trên CPU ===\n",
        "def load_and_resize_image(path, size=(512, 512)):\n",
        "    try:\n",
        "        if not os.path.exists(path):\n",
        "            return None\n",
        "        image = Image.open(path).convert(\"RGB\")\n",
        "        image = image.resize(size, Image.BILINEAR)\n",
        "        return ToTensor()(image)  # [3, H, W], float in [0,1]\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {path}:\", e)\n",
        "        return None\n",
        "\n",
        "# === Resize batch ảnh đã chuẩn hóa trên GPU ===\n",
        "def process_images_batch_gpu(image_paths, device=\"cuda\", target_size=(512, 512)):\n",
        "    batch_tensors = []\n",
        "    valid_paths = []\n",
        "\n",
        "    for path in image_paths:\n",
        "        tensor = load_and_resize_image(path, size=target_size)\n",
        "        if tensor is not None:\n",
        "            batch_tensors.append(tensor)\n",
        "            valid_paths.append(path)\n",
        "\n",
        "    if not batch_tensors:\n",
        "        return []\n",
        "\n",
        "    batch = torch.stack(batch_tensors).to(device)\n",
        "\n",
        "    # (Optional) Resize lại trên GPU nếu resize trên CPU không đủ chính xác\n",
        "    # batch = torch.nn.functional.interpolate(batch, size=target_size, mode='bilinear', align_corners=False)\n",
        "\n",
        "    pil_images = [tensor_to_pil(img) for img in batch]\n",
        "\n",
        "    # Cleanup (gọi ít để tránh chậm)\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return list(zip(valid_paths, pil_images))\n",
        "\n",
        "# === Tạo entry JSON từ batch ảnh + rows ===\n",
        "def make_dataset_entries_from_batch(rows, image_dict):\n",
        "    entries = []\n",
        "    for row in rows:\n",
        "        img_path = row.get(\"image\", \"\")\n",
        "        if img_path not in image_dict:\n",
        "            continue\n",
        "        entry = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": row.get(\"instruction\", \"\")},\n",
        "                    {\"type\": \"image\", \"image\": image_dict[img_path]}\n",
        "                ]\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": row.get(\"output\", \"\")}\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "        entries.append(entry)\n",
        "    return entries\n",
        "\n",
        "# === Xử lý toàn bộ dataset theo batch ===\n",
        "def process_dataset_batch(subset, batch_size=64, device=\"cuda\"):\n",
        "    new_dataset = []\n",
        "\n",
        "    for i in tqdm(range(0, len(subset), batch_size), desc=\"Processing in batches\"):\n",
        "        batch_rows_ds = subset.select(range(i, min(i + batch_size, len(subset))))\n",
        "        batch_rows = batch_rows_ds.to_list()\n",
        "\n",
        "        image_paths = [row.get(\"image\", \"\") for row in batch_rows]\n",
        "        image_pairs = process_images_batch_gpu(image_paths, device=device)\n",
        "\n",
        "        image_dict = {path: img for path, img in image_pairs}\n",
        "        batch_entries = make_dataset_entries_from_batch(batch_rows, image_dict)\n",
        "        new_dataset.extend(batch_entries)\n",
        "\n",
        "        # Gọi GC ít hơn, tránh làm chậm\n",
        "        if i % (batch_size * 5) == 0:\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return new_dataset\n",
        "\n",
        "# === Gọi main ===\n",
        "if __name__ == \"__main__\":\n",
        "    from datasets import load_dataset\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    BATCH_SIZE = 128  # Thử 128 trên A100 nếu ảnh nhỏ\n",
        "\n",
        "    # === Giả sử bạn đã load dataset từ HuggingFace hoặc file local ===\n",
        "    # dataset = load_dataset(\"path/to/your_dataset\")\n",
        "    # subset = dataset[\"train\"].select(range(1000))\n",
        "\n",
        "    # Debug test\n",
        "    # print(subset[0])  # kiểm tra một mẫu có image, instruction, output\n",
        "\n",
        "    start = time.perf_counter()\n",
        "    new_dataset = process_dataset_batch(subset, batch_size=BATCH_SIZE, device=device)\n",
        "    end = time.perf_counter()\n",
        "    print(f\"✅ Hoàn tất! Số lượng mẫu: {len(new_dataset)} | Thời gian: {end - start:.2f} giây\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAeQ9LXCAEkW"
      },
      "source": [
        "We can also render the LaTeX in the browser directly!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9CBpiISFa6C"
      },
      "source": [
        "To format the dataset, all vision finetuning tasks should be formatted as follows:\n",
        "\n",
        "```python\n",
        "[\n",
        "{ \"role\": \"user\",\n",
        "  \"content\": [{\"type\": \"text\",  \"text\": Q}, {\"type\": \"image\", \"image\": image} ]\n",
        "},\n",
        "{ \"role\": \"assistant\",\n",
        "  \"content\": [{\"type\": \"text\",  \"text\": A} ]\n",
        "},\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FY-9u-OD6_gE"
      },
      "source": [
        "Let's convert the dataset into the \"correct\" format for finetuning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aV3gzJPT6dzh"
      },
      "outputs": [],
      "source": [
        "len(new_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndDUB23CGAC5"
      },
      "source": [
        "We look at how the conversations are structured for the first example:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FecKS-dA82f5"
      },
      "source": [
        "Let's first see before we do any finetuning what the model outputs for the first example!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IR_zq_Pn6dzi"
      },
      "outputs": [],
      "source": [
        "image = new_dataset[0][0][\"content\"][1][\"image\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuun-Acb6dzi"
      },
      "outputs": [],
      "source": [
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcat4UxA81vr"
      },
      "outputs": [],
      "source": [
        "FastVisionModel.for_inference(model) # Enable for inference!\n",
        "\n",
        "instruction = \"Based on the paragraph describing the image and the article below describing\\nCNN s On the Road series brings you a greater insight into countries around the world. This time we travel to Azerbaijan in the lead up to the European Games to explore the culture of sports in the country sitting on the Caspian Sea. CNN Forget middle aged men in Lycra Azerbaijan s new breed of cyclist is young, fit and fast.Take Aqshin Ismailov and Tural Isgandarov.\\n\\nSporting the latest high performance bikes, they stand proudly in front of the futuristic Heydar Aliyev Cultural Centre in Baku, hoping to do their country proud in the European Games and other major competitions maybe even the Tour de France.Both are members of Azerbaijan s Baku Project, which has the express aim of developing cyclists capable of competing on the international stage.\\n\\nAs in other sports, the squad is a mix of local riders and imported talent with the common goal of winning top class bike races.Read MoreOpening honorThe 27 year old Ismailov will be under particular scrutiny on the opening weekend of the European Games, when he is the first Azeri athlete in action.Ismailov now specializes in mountain biking and the men s cross country event is one of the opening events on Saturday June 13, the first full day of competition.\\n\\nI will be the first to start and that s why it s a great feeling, he told CNN. My parents can be there to support me, and it s a big responsibility for me. Ismailov has set himself a modest goal against more experienced competitors from countries in which cycling is an established major sport.\\n\\nFrankly, I don t expect to get medal because I m a beginner in mountain biking, but I hope and believe that I will finish in the top 10, he predicted.Tour influenceIsgandarov, a road cyclist, believes the staging of a Tour of Azerbaijan cycle race for each of the last four years, attracting top class professionals, has helped massively raise the profile of the sport in his country. I come across many people who see the race on television and now come and watch our racing, he told CNN.\\n\\nFour years ago this sport wasn t popular, but interest in cycling is now growing day by day, he said.Big investmentNone of this has happened by accident, with Azerbaijan investing heavily in sports and hosting events like the European Games, and next year a Formula 1 race, in Baku. We are positioning ourselves as a sports country.\\n\\nNot to be focused on 10 sports or 15 sports but many more, Azerbaijan s Sport and Youth minister Azad Rahimov, told CNN.Local riders will face tough competition across all cycling disciplines at the European Games, with famous riders such as Tom Boonen of Belgium and Italy s Elia Viviani among the big names on the start sheets.But if the enthusiasm of Ismailov and Isgandarov can be translated into medals, they will also be challenging for places on the podium in the future.\\n\\nEvery small kid likes to ride a bike. For example, in my family we have three kids and they also want to be a cyclist. I say No problem. I ll help you. Ismailov, who used to compete in the national sport of wrestling before switching disciplines, agrees.\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": [\n",
        "        {\"type\": \"image\"},\n",
        "        {\"type\": \"text\", \"text\": instruction}\n",
        "    ]}\n",
        "]\n",
        "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt = True)\n",
        "inputs = tokenizer(\n",
        "    image,\n",
        "    input_text,\n",
        "    add_special_tokens = False,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 512,\n",
        "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!\n",
        "\n",
        "We use our new `UnslothVisionDataCollator` which will help in our vision finetuning setup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from unsloth import is_bf16_supported\n",
        "from unsloth.trainer import UnslothVisionDataCollator\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "FastVisionModel.for_training(model) # Enable for training!\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    data_collator = UnslothVisionDataCollator(model, tokenizer), # Must use!\n",
        "    train_dataset = new_dataset,\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 16,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 10,\n",
        "        num_train_epochs = 3, # Set this instead of max_steps for full training runs\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bf16_supported(),\n",
        "        bf16 = is_bf16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\",     # For Weights and Biases\n",
        "\n",
        "        # You MUST put the below items for vision finetuning:\n",
        "        remove_unused_columns = False,\n",
        "        dataset_text_field = \"\",\n",
        "        dataset_kwargs = {\"skip_prepare_dataset\": True},\n",
        "        dataset_num_proc = 4,\n",
        "        max_seq_length = 2048,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2ejIt2xSNKKp"
      },
      "outputs": [],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKOgKWXb6dzi"
      },
      "source": [
        "###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqxqAZ7KJ4oL"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pCqnaKmlO1U9"
      },
      "outputs": [],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! You can change the instruction and input - leave the output blank!\n",
        "\n",
        "We use `min_p = 0.1` and `temperature = 1.5`. Read this [Tweet](https://x.com/menhguin/status/1826132708508213629) for more information on why."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR3gIAX-SM2q"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "file_path = '/content/drive/MyDrive/eventa/test1000tokenQwen.json'\n",
        "df = pd.read_json(file_path, lines=True)\n",
        "dff = df[1000:2000]\n",
        "image_files = df['image'].tolist()\n",
        "image_files = image_files[1000:2000]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIL8sinqxv5e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import json\n",
        "import gc\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# Hàm resize và chuyển ảnh sang PNG trong bộ nhớ\n",
        "def pil_to_png_list(pil_img):\n",
        "    pil_img = pil_img.resize((512, 512), resample=Image.Resampling.LANCZOS)\n",
        "    buffer = BytesIO()\n",
        "    pil_img.save(buffer, format=\"PNG\")\n",
        "    buffer.seek(0)\n",
        "    img = Image.open(buffer).convert(\"RGB\")\n",
        "    return [img]\n",
        "\n",
        "# Cấu hình thư mục và file output\n",
        "image_dir = \"/content/drive/MyDrive/eventa/query\"\n",
        "batch_size = 1\n",
        "\n",
        "\n",
        "results = []\n",
        "\n",
        "# Hàm sinh caption từ ảnh\n",
        "def generate_caption(image_path, instruction):\n",
        "    pil_image = Image.open(image_path).convert(\"RGB\")\n",
        "    image_list = pil_to_png_list(pil_image)\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": image_list[0]},\n",
        "                {\"type\": \"text\", \"text\": instruction}\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        image_list[0],\n",
        "        input_text,\n",
        "        add_special_tokens=False,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=512,\n",
        "            use_cache=True,\n",
        "            temperature=1.5,\n",
        "            do_sample=False,\n",
        "            top_p=0.9\n",
        "        )\n",
        "\n",
        "    caption = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    results.append({\n",
        "        \"filename\": os.path.basename(image_path),\n",
        "        \"caption\": caption.strip(),\n",
        "    })\n",
        "\n",
        "    del inputs, output\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "for i in tqdm(range(0, len(image_files), batch_size), desc=\"Processing batches\"):\n",
        "    batch_files = image_files[i:i + batch_size]\n",
        "\n",
        "    for j, filename in enumerate(batch_files):\n",
        "        image_path = os.path.join(image_dir, filename)\n",
        "        instruction = dff.iloc[i + j]['instruction']\n",
        "        generate_caption(image_path, instruction)\n",
        "output_file = \"/content/drive/MyDrive/eventa/image_captions1.json\"\n",
        "# Lưu kết quả ra file JSON\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(results, f, ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "\n",
        "from huggingface_hub import login\n",
        "login(HF_TOKEN)\n",
        "\n",
        "model.push_to_hub(\"Nguyenhhh/Qwen-400M\")\n",
        "tokenizer.push_to_hub(\"Nguyenhhh/Qwen-400M\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyYr15812UjH"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('secretName')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKX_XKs_BNZR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M68kBQ4b6dzk"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Thư mục gốc cần nén\n",
        "base_dir = '/kaggle/working'\n",
        "zip_filename = '/kaggle/working/working_directory.zip'\n",
        "\n",
        "# Tạo file zip mới\n",
        "with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "    for root, dirs, files in os.walk(base_dir):\n",
        "        for file in files:\n",
        "            abs_path = os.path.join(root, file)\n",
        "            # Giữ nguyên cấu trúc từ gốc /\n",
        "            arcname = abs_path.lstrip('/')  # hoặc dùng arcname = os.path.relpath(abs_path, '/') để chắc chắn\n",
        "            zipf.write(abs_path, arcname=arcname)\n",
        "\n",
        "print(f\"Đã tạo file zip: {zip_filename}\")\n",
        "from IPython.display import FileLink\n",
        "FileLink(\"/kaggle/working/working_directory.zip\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "cell_execution_strategy": "setup",
      "gpuType": "A100",
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 7549400,
          "sourceId": 12001149,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 7557937,
          "sourceId": 12013560,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 7578136,
          "sourceId": 12042829,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 7599088,
          "sourceId": 12072129,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31040,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
